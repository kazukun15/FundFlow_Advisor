import io, re
import streamlit as st
import pdfplumber
import pandas as pd
import pytesseract
from pdf2image import convert_from_bytes
import google.generativeai as genai

# â”€â”€â”€ Streamlit Secrets ã‹ã‚‰ google.api_key ã‚’å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "google" not in st.secrets or "api_key" not in st.secrets["google"]:
    st.error('`.streamlit/secrets.toml` ã«ä»¥ä¸‹ã‚’ç™»éŒ²ã—ã¦ãã ã•ã„:\n\n'
             '[google]\n'
             'api_key = "YOUR_GOOGLE_API_KEY"')
    st.stop()
genai.configure(api_key=st.secrets["google"]["api_key"])

# â”€â”€â”€ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_unique(cols):
    seen = {}; out = []
    for c in cols:
        cnt = seen.get(c, 0)
        name = f"{c}.{cnt}" if cnt else c
        seen[c] = cnt + 1
        out.append(name)
    return out

def extract_tables_from_pdf(buf: bytes) -> pd.DataFrame:
    with pdfplumber.open(io.BytesIO(buf)) as pdf:
        tbls = []
        for p in pdf.pages:
            for t in p.extract_tables() or []:
                if len(t) > 1:
                    df = pd.DataFrame(t[1:], columns=t[0])
                    df.columns = make_unique([str(c).strip() for c in df.columns])
                    tbls.append(df)
    if not tbls:
        return pd.DataFrame()
    cols = sorted({c for df in tbls for c in df.columns})
    aligned = [df.reindex(columns=cols) for df in tbls]
    return pd.concat(aligned, ignore_index=True, sort=False)

def ocr_text(buf: bytes) -> str:
    imgs = convert_from_bytes(buf)
    return "\n".join(pytesseract.image_to_string(img, lang="jpn") for img in imgs)

def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [re.sub(r"\s+", "", str(c)) for c in df.columns]
    for c in df.columns:
        s = (
            df[c].astype(str)
                 .map(lambda x: re.sub(r"[^\d\.\-]", "", x))
                 .replace({"": pd.NA})
        )
        df[c] = pd.to_numeric(s, errors="ignore")
    df.dropna(axis=0, how="all", inplace=True)
    df.dropna(axis=1, how="all", inplace=True)
    return df

def reconcile(pub: pd.DataFrame, others: dict) -> pd.DataFrame:
    base = pub.select_dtypes("number").sum().sum()
    rows = []
    for name, df in others.items():
        tot = df.select_dtypes("number").sum().sum()
        rows.append({"ãƒ¬ãƒãƒ¼ãƒˆ": name, "å…¬é‡‘æ—¥è¨ˆåˆè¨ˆ": base, "ä»–æ—¥å ±åˆè¨ˆ": tot, "å·®ç•°": tot - base})
    return pd.DataFrame(rows)

def analyze_cf(pub: pd.DataFrame):
    ob_series = pub.get("ä¼šè¨ˆå‰æ—¥æ®‹é«˜", pd.Series(dtype=float)).dropna()
    ob = ob_series.iloc[0] if not ob_series.empty else None
    nums = pub.select_dtypes("number")
    sums = nums.sum().rename("åˆè¨ˆ").to_frame()
    if {"å…¥é‡‘", "å‡ºé‡‘"}.issubset(nums.columns):
        infl, outf = nums["å…¥é‡‘"].sum(), nums["å‡ºé‡‘"].sum()
    elif {"åå…¥", "æ”¯å‡º"}.issubset(nums.columns):
        infl, outf = nums["åå…¥"].sum(), nums["æ”¯å‡º"].sum()
    else:
        infl = nums[nums > 0].sum().sum()
        outf = -nums[nums < 0].sum().sum()
    net = infl - outf
    return {"å‰æ—¥æ®‹é«˜": ob, "ç·æµå…¥": infl, "ç·æµå‡º": outf, "ç´”å¢—æ¸›": net}, sums

def ai_suggest(df_diff: pd.DataFrame) -> str:
    diff = df_diff[df_diff["å·®ç•°"] != 0]
    txt = diff.to_string(index=False) if not diff.empty else "ï¼ˆå·®ç•°ãªã—ï¼‰"
    prompt = f"ä»¥ä¸‹ã®å·®ç•°ã«ã¤ã„ã¦ã€åŸå› ã‚’ç®‡æ¡æ›¸ãã§ç¤ºã—ã¦ãã ã•ã„ã€‚\n\n{txt}"
    try:
        model = genai.GenerativeModel("gemini-2.5-flash-preview-04-17")
        resp  = model.generate_content(prompt)
        if hasattr(resp, "text") and resp.text:
            return resp.text.strip()
        if hasattr(resp, "parts"):
            return "".join(p.text for p in resp.parts).strip()
        st.warning("AIå¿œç­”å½¢å¼ãŒäºˆæœŸã›ã¬ã‚‚ã®ã§ã—ãŸã€‚")
        return "AIã‹ã‚‰æœ‰åŠ¹ãªææ¡ˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except Exception as e:
        st.error(f"AIææ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "AIææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def fund_advice(pub: pd.DataFrame) -> (dict, str):
    # ã€ŒåŸºé‡‘ã€ã‚’å«ã‚€åˆ—ã‚’æ¤œå‡º
    fund_cols = [c for c in pub.columns if "åŸºé‡‘" in c]
    if not fund_cols:
        return {}, "åŸºé‡‘ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    # é‡‘é¡åˆè¨ˆ
    fund_sums = {c: pub[c].sum() for c in fund_cols if pd.api.types.is_numeric_dtype(pub[c])}
    # AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    table = "\n".join(f"{c}: {v:,}" for c, v in fund_sums.items())
    prompt = (
        "ä»¥ä¸‹ã¯åŸºé‡‘ã®æ®‹é«˜æƒ…å ±ã§ã™ã€‚\n"
        f"{table}\n"
        "ã“ã‚Œã‚‰ã‚’ä¸€èˆ¬ä¼šè¨ˆã¸ç¹°ã‚Šå…¥ã‚Œã‚‹ã€ã¾ãŸã¯å„Ÿé‚„ã«åˆ©ç”¨ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¨ãŠã™ã™ã‚ã®æ–¹æ³•ã‚’ã€"
        "ç®‡æ¡æ›¸ãã§3ï½5ç‚¹æŒ™ã’ã¦ãã ã•ã„ã€‚"
    )
    try:
        model = genai.GenerativeModel("gemini-2.5-flash-preview-04-17")
        resp  = model.generate_content(prompt)
        if hasattr(resp, "text") and resp.text:
            advice = resp.text.strip()
        elif hasattr(resp, "parts"):
            advice = "".join(p.text for p in resp.parts).strip()
        else:
            advice = "AIã‹ã‚‰åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except Exception as e:
        st.error(f"åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        advice = "åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    return fund_sums, advice

# â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.set_page_config(page_title="FundFlow Advisor AI", layout="wide")
    st.title("FundFlow Advisor AI")
    st.markdown("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€çªåˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†æãƒ»AIææ¡ˆãƒ»åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¡Œã„ã¾ã™ã€‚")

    files = st.sidebar.file_uploader(
        "ğŸ“ å…¬é‡‘æ—¥è¨ˆPDF ã¨ ä»–æ—¥å ±PDFï¼ˆè¤‡æ•°å¯ï¼‰",
        type=None, accept_multiple_files=True
    )
    if not files:
        st.sidebar.info("ã“ã“ã«PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    pub_df = pd.DataFrame()
    others = {}
    for f in files:
        if not f.name.lower().endswith(".pdf"):
            st.sidebar.error(f"{f.name} ã¯PDFã§ã¯ã‚ã‚Šã¾ã›ã‚“"); continue
        buf = f.read()
        df  = extract_tables_from_pdf(buf)
        if df.empty:
            st.sidebar.warning(f"{f.name} è¡¨æŠ½å‡ºå¤±æ•—â†’OCRè¡¨ç¤º")
            st.sidebar.text_area(f"OCR({f.name})", ocr_text(buf), height=150)
            continue
        df = normalize_df(df)
        if pub_df.empty:
            pub_df = df; st.sidebar.success(f"{f.name} ã‚’å…¬é‡‘æ—¥è¨ˆã«è¨­å®š")
        else:
            others[f.name] = df; st.sidebar.success(f"{f.name} ã‚’ä»–æ—¥å ±ã«è¿½åŠ ")

    if pub_df.empty:
        st.warning("å…¬é‡‘æ—¥è¨ˆPDFãŒå¿…è¦ã§ã™ã€‚")
        return

    df_diff    = reconcile(pub_df, others) if others else pd.DataFrame()
    cf_metrics, cf_sums = analyze_cf(pub_df)
    diff_ai    = ai_suggest(df_diff)     if not df_diff.empty else "ä»–æ—¥å ±ãŒãªã„ãŸã‚çªåˆAIã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    fund_sums, fund_ai = fund_advice(pub_df)

    tab1, tab2, tab3, tab4, tab5 = st.tabs(
        ["ğŸ” ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼","ğŸ“Š å·®ç•°ã‚µãƒãƒªãƒ¼","ğŸ’¡ åˆ†æ","ğŸ¤– AIçªåˆææ¡ˆ","ğŸ¦ åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹"]
    )

    with tab1:
        st.subheader("â–  å…¬é‡‘æ—¥è¨ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(pub_df, use_container_width=True)
        if others:
            st.subheader("â–  ä»–æ—¥å ±ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            for name, df in others.items():
                st.markdown(f"**{name}**")
                st.dataframe(df, use_container_width=True)

    with tab2:
        st.subheader("â–  å·®ç•°ã‚µãƒãƒªãƒ¼")
        if df_diff.empty:
            st.info("ä»–æ—¥å ±ãŒãªã„ãŸã‚å·®ç•°ã‚µãƒãƒªãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.dataframe(df_diff, use_container_width=True)

    with tab3:
        st.subheader("â–  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼åˆ†æ")
        if cf_metrics["å‰æ—¥æ®‹é«˜"] is not None:
            st.metric("å‰æ—¥æ®‹é«˜", f"{int(cf_metrics['å‰æ—¥æ®‹é«˜']):,}")
        c1, c2, c3 = st.columns(3)
        c1.metric("ç·æµå…¥", f"{int(cf_metrics['ç·æµå…¥']):,}")
        c2.metric("ç·æµå‡º", f"{int(cf_metrics['ç·æµå‡º']):,}")
        c3.metric("ç´”å¢—æ¸›", f"{int(cf_metrics['ç´”å¢—æ¸›']):,}")
        st.subheader("â–ª åˆ—åˆè¨ˆãƒ‡ãƒãƒƒã‚°")
        st.dataframe(cf_sums, use_container_width=True)
        if cf_metrics["ç´”å¢—æ¸›"] < 0:
            st.error("âš ï¸ è³‡é‡‘ã‚·ãƒ§ãƒ¼ãƒˆãƒªã‚¹ã‚¯")
        elif cf_metrics["ç´”å¢—æ¸›"] < cf_metrics["ç·æµå‡º"] * 0.1:
            st.warning("âš ï¸ è³‡é‡‘ä½™è£•ãŒä¹ã—ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        else:
            st.success("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¸ã‚·ãƒ§ãƒ³ã¯å¥å…¨ã§ã™")

    with tab4:
        st.subheader("â–  AIã«ã‚ˆã‚‹çªåˆåŸå› ææ¡ˆ")
        st.markdown(diff_ai)

    with tab5:
        st.subheader("â–  åŸºé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
        if fund_sums:
            st.table(pd.DataFrame.from_dict(fund_sums, orient="index", columns=["æ®‹é«˜"]).assign(æ®‹é«˜=lambda df: df["æ®‹é«˜"].map("{:,}".format)))
        else:
            st.info("åŸºé‡‘ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.markdown(fund_ai)

if __name__ == "__main__":
    main()
